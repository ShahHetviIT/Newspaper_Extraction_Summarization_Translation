{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c08856-82ff-42a3-9d87-9d8bd8e15e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_news(api_key, selected_date,language):\n",
    "    url = 'https://newsapi.org/v2/everything'\n",
    "    \n",
    "    # Format selected_date to match NewsAPI's date format\n",
    "    selected_date_str = datetime.strptime(selected_date, '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Define the parameters for the API request\n",
    "    params = {\n",
    "        'q': 'India',  # Broad keyword to fetch news\n",
    "        'from': selected_date_str,  # Start date\n",
    "        'to': selected_date_str,    # End date\n",
    "        'apiKey': api_key,\n",
    "        'language': language \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching news: {e}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Function to clean the description\n",
    "    def clean_description(description):\n",
    "        clean_text = re.sub(r'<[^>]+>', '', description)\n",
    "        clean_text = re.sub(r'\\[\\d+\\s\\w+\\]', '', clean_text)\n",
    "        clean_text = re.sub(r'\\[.*?\\]', '', clean_text)\n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "        clean_text = clean_text.strip()\n",
    "        return clean_text\n",
    "\n",
    "    # Function to scrape article content\n",
    "    def scrape_article_content(url):\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        try:\n",
    "            article_response = requests.get(url, headers=headers)\n",
    "            article_response.raise_for_status()\n",
    "            soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "\n",
    "            article_body = soup.find('main') or soup.find('article')\n",
    "            if article_body:\n",
    "                paragraphs = article_body.find_all('p')\n",
    "                content = ' '.join(p.get_text() for p in paragraphs)\n",
    "                \n",
    "                # Clean up extra spaces and newlines\n",
    "                content = re.sub(r'\\s+', ' ', content).strip()\n",
    "                \n",
    "                # Ensure content length is between 0 and 5000 characters\n",
    "                if len(content) > 5000:\n",
    "                    content = content[:5000]\n",
    "                elif len(content) == 0:\n",
    "                    return \"\"\n",
    "                \n",
    "                # Ensure content ends with a full stop if not empty\n",
    "                if content and not content.endswith('.'):\n",
    "                    content += '.'\n",
    "                \n",
    "                return content\n",
    "            return \"\"\n",
    "        except requests.RequestException:\n",
    "            # Return empty string for any request-related errors\n",
    "            return \"\"\n",
    "\n",
    "    # Preprocess and filter the news data\n",
    "    def preprocess_news(news_list):\n",
    "        valid_news = []\n",
    "        urls = [article.get('url', '').strip() for article in news_list]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            future_to_url = {executor.submit(scrape_article_content, url): url for url in urls}\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    full_content = future.result()\n",
    "                    if full_content:  # Only process if content is not empty\n",
    "                        for article in news_list:\n",
    "                            if article.get('url', '').strip() == url:\n",
    "                                title = article.get('title', '').strip()\n",
    "                                published = article.get('publishedAt', '').strip()\n",
    "                                description = article.get('description', '').strip()\n",
    "                                \n",
    "                                cleaned_description = clean_description(description)\n",
    "                                \n",
    "                                if title and url and published != '1970-01-01T00:00:00Z':\n",
    "                                    try:\n",
    "                                        date_published = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d')\n",
    "                                        if date_published == selected_date:\n",
    "                                            valid_news.append({\n",
    "                                                'title': title,\n",
    "                                                'link': url,\n",
    "                                                'date': date_published,\n",
    "                                                'short_description': cleaned_description,\n",
    "                                                'content': full_content\n",
    "                                            })\n",
    "                                    except ValueError:\n",
    "                                        continue\n",
    "                except Exception:\n",
    "                    # If there's an issue with the future result, simply continue\n",
    "                    continue\n",
    "\n",
    "        return valid_news\n",
    "\n",
    "    news_list = data.get('articles', [])\n",
    "    filtered_news = preprocess_news(news_list)\n",
    "    \n",
    "    return filtered_news\n",
    "\n",
    "# Example usage\n",
    "api_key = '95bb6bf55ec742bca7cfeb768f50245f'  # Replace with your actual NewsAPI key\n",
    "selected_date = '2024-09-13'\n",
    "language = 'en'\n",
    "data = get_news(api_key, selected_date,language)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5acbe65-5808-48b6-b6b8-292cbf065c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "clf_loaded_model = pickle.load(open('D:\\\\sem 7\\\\Newspaper_Extraction_Summarization_Translation\\\\backend\\\\clf_model','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e71c3e-4e3b-4546-9262-37cccc3ebd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categories(docs):\n",
    "    \"\"\"\n",
    "    Predict the categories for a list of news articles.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list): List of news articles as strings.\n",
    "\n",
    "    Returns:\n",
    "    list: List of predicted categories.\n",
    "    \"\"\"\n",
    "    predicted = clf_loaded_model.predict(docs)\n",
    "    return predicted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f492d-99df-4883-a15f-b1dd823e19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_news_by_category(docs, categories, selected_category):\n",
    "    \"\"\"\n",
    "    Filter and display news articles based on the selected category.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list): List of news articles as strings.\n",
    "    categories (list): List of predicted categories corresponding to the docs.\n",
    "    selected_category (str): The category to filter news articles by.\n",
    "\n",
    "    Returns:\n",
    "    list: List of news articles belonging to the selected category.\n",
    "    \"\"\"\n",
    "    filtered_news = [doc for doc, category in zip(docs, categories) if category == selected_category]\n",
    "    return filtered_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941605a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951bf56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480f4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from threading import Thread\n",
    "import spacy\n",
    "from heapq import nlargest\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from flask_cors import CORS\n",
    "\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Welcome to the News API!!!!!!\"\n",
    "\n",
    "@app.route('/api/get-news', methods=['GET'])\n",
    "def fetch_news():\n",
    "    api_key = request.args.get('api_key')\n",
    "    selected_date = request.args.get('date')\n",
    "    language = request.args.get('language', 'en')  # Default to English if not provided\n",
    "    \n",
    "    if not api_key or not selected_date:\n",
    "        return jsonify({\"error\": \"API key and date are required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Fetch and process news data\n",
    "        news_data = get_news(api_key, selected_date, language)\n",
    "        \n",
    "        # Save the processed news data to the database\n",
    "        store_news_to_db(news_data)\n",
    "        \n",
    "        return jsonify(news_data)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/api/predict-categories', methods=['GET'])\n",
    "def predict_categories_endpoint():\n",
    "    try:\n",
    "        # Load news data from the database\n",
    "        df = load_news_from_db()\n",
    "        docs_new = df['content'].tolist()\n",
    "        \n",
    "        # Predict categories\n",
    "        predicted_categories = predict_categories(docs_new)\n",
    "        unique_categories = list(set(predicted_categories))\n",
    "        \n",
    "        return jsonify({\"categories\": unique_categories})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/api/filter_news', methods=['GET'])\n",
    "def filter_news_endpoint():\n",
    "    selected_category = request.args.get('category')\n",
    "    \n",
    "    if not selected_category:\n",
    "        return jsonify({\"error\": \"Category parameter is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Load news data from the database\n",
    "        df = load_news_from_db()\n",
    "        docs_new = df['content'].tolist()\n",
    "        predicted_categories = predict_categories(docs_new)\n",
    "        \n",
    "        # Convert the selected category to uppercase to match your case-sensitive data\n",
    "#         selected_category = selected_category.upper()\n",
    "        \n",
    "        # Filter news by the selected category (case-insensitive)\n",
    "        filtered_news = filter_news_by_category(docs_new, predicted_categories, selected_category)\n",
    "        \n",
    "        return jsonify({\"news\": filtered_news})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/api/summarize-news', methods=['GET'])\n",
    "def summarize_news():\n",
    "    title = request.args.get('title')\n",
    "    \n",
    "    if not title:\n",
    "        return jsonify({\"error\": \"Title parameter is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Load news data from the database\n",
    "        df = load_news_from_db()\n",
    "        selected_news = df[df['title'] == title]\n",
    "\n",
    "        if selected_news.empty:\n",
    "            return jsonify({\"error\": \"News article not found\"}), 404\n",
    "\n",
    "        content = selected_news['content'].values[0]\n",
    "\n",
    "        # Summarize the content\n",
    "        doc = nlp(content)\n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        word_frequencies = {}\n",
    "        for word in doc:\n",
    "            if word.text.lower() not in stopwords and word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "        max_frequency = max(word_frequencies.values())\n",
    "        for word in word_frequencies.keys():\n",
    "            word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "        sentence_tokens = [sent for sent in doc.sents]\n",
    "        sentence_scores = {}\n",
    "        for sent in sentence_tokens:\n",
    "            for word in sent:\n",
    "                if word.text.lower() in word_frequencies.keys():\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "        select_length = int(len(sentence_tokens) * 0.3)\n",
    "        summary_sentences = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "        final_summary = [sent.text for sent in summary_sentences]\n",
    "        summary = ' '.join(final_summary)\n",
    "        \n",
    "        insert_summary_into_db(title,summary)\n",
    "\n",
    "        return jsonify({\"summary\": summary})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "    \n",
    "@app.route('/api/translate-news', methods=['GET'])\n",
    "def translate_news():\n",
    "    title = request.args.get('title')\n",
    "    target_language = request.args.get('language', 'hi')  # Default to Hindi if no language is provided\n",
    "    use_summarized = request.args.get('summarized', 'false').lower() == 'true'  # Check if summarized content should be used\n",
    "\n",
    "    if not title:\n",
    "        return jsonify({\"error\": \"Title parameter is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Load news data from the database\n",
    "        if use_summarized:\n",
    "            # Load summary data from the summary table\n",
    "            summary_df = load_summary_from_db()\n",
    "            selected_summary = summary_df[summary_df['title'] == title]\n",
    "\n",
    "            if selected_summary.empty:\n",
    "                return jsonify({\"error\": \"Summary not found for the specified title\"}), 404\n",
    "\n",
    "            # Fetch the summarized content\n",
    "            content = selected_summary['summary'].values[0]  # Use summarized content\n",
    "        else:\n",
    "            # Load the original news data from the news table\n",
    "            news_df = load_news_from_db()\n",
    "            selected_news = news_df[news_df['title'] == title]\n",
    "\n",
    "            if selected_news.empty:\n",
    "                return jsonify({\"error\": \"News article not found\"}), 404\n",
    "\n",
    "            # Fetch the original content\n",
    "            content = selected_news['content'].values[0]  # Use original content\n",
    "\n",
    "        # Ensure content is valid and not None or empty\n",
    "        if not content:\n",
    "            return jsonify({\"error\": \"Content for the selected news article is empty\"}), 404\n",
    "\n",
    "        # Translate the content to the desired language\n",
    "        try:\n",
    "            translated = translator.translate(content, dest=target_language)\n",
    "        except Exception as translation_error:\n",
    "            return jsonify({\"error\": f\"Translation failed: {str(translation_error)}\"}), 500\n",
    "\n",
    "        return jsonify({\"translated_content\": translated.text})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "def store_news_to_db(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty:\n",
    "        print(\"No data to store.\")\n",
    "        return\n",
    "\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='mysqlpassword',\n",
    "        database='newsdb'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"TRUNCATE TABLE news_details\")\n",
    "            for _, row in df.iterrows():\n",
    "                sql = \"\"\"\n",
    "                    INSERT INTO news_details (title, link, date, short_description, content)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "                cursor.execute(sql, (row['title'], row['link'], row['date'], row['short_description'], row['content']))\n",
    "            connection.commit()\n",
    "            print(\"Data successfully stored in the database.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing data to database: {e}\")\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "def load_news_from_db():\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='mysqlpassword',\n",
    "        database='newsdb'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        query = \"SELECT * FROM news_details\"\n",
    "        df = pd.read_sql(query, connection)\n",
    "        return df\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "        \n",
    "def insert_summary_into_db(title, summarized_content):\n",
    "    \"\"\"Insert the title and summarized content into the 'news_summ' table.\"\"\"\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='mysqlpassword',\n",
    "        database='newsdb'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"\"\"\n",
    "                INSERT INTO news_table (title, summary)\n",
    "                VALUES (%s, %s)\n",
    "            \"\"\"\n",
    "            cursor.execute(sql, (title, summarized_content))\n",
    "            connection.commit()\n",
    "            print(f\"Summarized content stored for title: {title}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing summarized content: {e}\")\n",
    "    finally:\n",
    "        connection.close()\n",
    "        \n",
    "def load_summary_from_db():\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='mysqlpassword',\n",
    "        database='newsdb'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        query = \"SELECT title, summary FROM news_table\"  # Assuming your summary table is named 'news_summ'\n",
    "        df = pd.read_sql(query, connection)\n",
    "        return df\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "def run_flask_app():\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "\n",
    "# Run the app in a separate thread so Jupyter doesn't block\n",
    "flask_thread = Thread(target=run_flask_app)\n",
    "flask_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba70cb7-b1e6-4a74-8e75-f5d77f113d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
